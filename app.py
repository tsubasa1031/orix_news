import streamlit as st
import pandas as pd
import requests
from bs4 import BeautifulSoup
import datetime

# ãƒšãƒ¼ã‚¸è¨­å®š
st.set_page_config(
    page_title="ã‚ªãƒªãƒƒã‚¯ã‚¹ãƒ»ãƒãƒ•ã‚¡ãƒ­ãƒ¼ã‚º ãƒ‹ãƒ¥ãƒ¼ã‚¹ã¾ã¨ã‚",
    page_icon="âš¾",
    layout="wide"
)

# --- 1. ãƒ‡ãƒ¼ã‚¿å–å¾—é–¢æ•° (Google News RSSã‹ã‚‰å–å¾—) ---
@st.cache_data(ttl=1800)  # 30åˆ†é–“ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã—ã¦ã‚¢ã‚¯ã‚»ã‚¹è² è·ã‚’è»½æ¸›
def load_data():
    # Google News RSSæ¤œç´¢ (ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: ã‚ªãƒªãƒƒã‚¯ã‚¹ ãƒãƒ•ã‚¡ãƒ­ãƒ¼ã‚º)
    # hl=ja&gl=JP&ceid=JP:ja ã§æ—¥æœ¬ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’æŒ‡å®š
    url = "https://news.google.com/rss/search?q=ã‚ªãƒªãƒƒã‚¯ã‚¹+ãƒãƒ•ã‚¡ãƒ­ãƒ¼ã‚º&hl=ja&gl=JP&ceid=JP:ja"
    
    try:
        response = requests.get(url, timeout=10)
        response.raise_for_status()
        
        # XMLã¨ã—ã¦ãƒ‘ãƒ¼ã‚¹ (features="xml" ã‚’ä½¿ç”¨ã™ã‚‹ã«ã¯ lxml ãŒå¿…è¦)
        # lxmlãŒãªã„ç’°å¢ƒã®å ´åˆã¯ "html.parser" ã§ã‚‚ä»£ç”¨å¯èƒ½ã§ã™ãŒã€xmlæ¨å¥¨
        soup = BeautifulSoup(response.content, "xml")
        items = soup.find_all("item")
        
        news_list = []
        for item in items:
            title = item.title.text
            link = item.link.text
            pub_date_str = item.pubDate.text
            description = item.description.text
            
            # æ—¥ä»˜ã®ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›
            # RSSã®æ—¥ä»˜å½¢å¼ (ä¾‹: Wed, 26 Nov 2025 ...) ã‚’æ‰±ã„ã‚„ã™ãå¤‰æ›
            try:
                pub_date = pd.to_datetime(pub_date_str).strftime('%Y-%m-%d %H:%M')
            except:
                pub_date = pub_date_str

            # descriptionã«ã¯HTMLãŒå«ã¾ã‚Œã‚‹å ´åˆãŒã‚ã‚‹ãŸã‚ã€ãƒ†ã‚­ã‚¹ãƒˆã®ã¿æŠ½å‡ºã—ã¦è¦ç´„ã‚’ä½œæˆ
            summary_soup = BeautifulSoup(description, "html.parser")
            summary_text = summary_soup.get_text()[:80] + "..." if summary_soup.get_text() else "è©³ç´°ã¯ã‚ã‚Šã¾ã›ã‚“"

            # ãƒ‹ãƒ¥ãƒ¼ã‚¹æä¾›å…ƒã‚’ã‚¿ã‚¤ãƒˆãƒ«ã‹ã‚‰æŠ½å‡º (Google Newsã®å½¢å¼: "ã‚¿ã‚¤ãƒˆãƒ« - æä¾›å…ƒ")
            source = "News"
            if " - " in title:
                parts = title.rsplit(" - ", 1)
                title = parts[0] # è¨˜äº‹ã‚¿ã‚¤ãƒˆãƒ«
                source = parts[1] # æä¾›å…ƒ (ä¾‹: Yahoo!ãƒ‹ãƒ¥ãƒ¼ã‚¹, æ—¥åˆŠã‚¹ãƒãƒ¼ãƒ„)

            news_list.append({
                "date": pub_date,
                "category": source, # æä¾›å…ƒã‚’ã‚«ãƒ†ã‚´ãƒªã¨ã—ã¦åˆ©ç”¨
                "title": title,
                "summary": summary_text,
                "link": link,
                "tags": ["Webè¨˜äº‹"]
            })
            
        return pd.DataFrame(news_list)

    except Exception as e:
        st.error(f"ãƒ‹ãƒ¥ãƒ¼ã‚¹ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
        # ã‚¨ãƒ©ãƒ¼æ™‚ã®ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿
        return pd.DataFrame([
            {"date": "-", "category": "Error", "title": "ãƒ‡ãƒ¼ã‚¿å–å¾—ã‚¨ãƒ©ãƒ¼", "summary": "å†èª­ã¿è¾¼ã¿ã—ã¦ãã ã•ã„ã€‚", "link": "#", "tags": []}
        ])

df = load_data()

# --- 2. ã‚µã‚¤ãƒ‰ãƒãƒ¼ (ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°) ---
st.sidebar.title("ğŸ” æ¤œç´¢ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼")

if not df.empty:
    # ãƒ‹ãƒ¥ãƒ¼ã‚¹æä¾›å…ƒï¼ˆãƒ¡ãƒ‡ã‚£ã‚¢ï¼‰ã§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
    categories = df["category"].unique()
    selected_categories = st.sidebar.multiselect(
        "ãƒ¡ãƒ‡ã‚£ã‚¢ã§çµã‚Šè¾¼ã¿",
        categories,
        default=categories
    )
    
    # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢
    search_query = st.sidebar.text_input("ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢ (ä¾‹: å¥‘ç´„æ›´æ”¹)")
    
    # ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å®Ÿè¡Œ
    filtered_df = df[df["category"].isin(selected_categories)]
    
    if search_query:
        filtered_df = filtered_df[
            filtered_df["title"].str.contains(search_query, case=False) | 
            filtered_df["summary"].str.contains(search_query, case=False)
        ]
else:
    filtered_df = df

# --- 3. ãƒ¡ã‚¤ãƒ³ç”»é¢ ---
st.title("âš¾ ã‚ªãƒªãƒƒã‚¯ã‚¹ãƒ»ãƒãƒ•ã‚¡ãƒ­ãƒ¼ã‚º æœ€æ–°ãƒ‹ãƒ¥ãƒ¼ã‚¹")
st.caption("Google News RSSã‚ˆã‚Šè‡ªå‹•åé›†")

# æ›´æ–°ãƒœã‚¿ãƒ³ (ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã‚¯ãƒªã‚¢ã—ã¦å†å–å¾—)
if st.button("ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’æ›´æ–°"):
    load_data.clear()
    st.rerun()

st.markdown(f"æœ€æ–°è¨˜äº‹: **{len(filtered_df)}** ä»¶")

# è¡¨ç¤ºãƒ¢ãƒ¼ãƒ‰åˆ‡ã‚Šæ›¿ãˆ
view_mode = st.radio("è¡¨ç¤ºå½¢å¼:", ["ã‚«ãƒ¼ãƒ‰è¡¨ç¤º", "ãƒ‡ãƒ¼ã‚¿ãƒ†ãƒ¼ãƒ–ãƒ«"], horizontal=True)
st.divider()

if not filtered_df.empty:
    if view_mode == "ã‚«ãƒ¼ãƒ‰è¡¨ç¤º":
        for index, row in filtered_df.iterrows():
            # æä¾›å…ƒã‚’è¦‹å‡ºã—ã«å«ã‚ã¦Expanderã‚’ä½œæˆ
            with st.expander(f"ã€{row['category']}ã€‘ {row['title']}", expanded=True):
                st.caption(f"ğŸ“… {row['date']}")
                st.write(row['summary'])
                # ãƒªãƒ³ã‚¯ãƒœã‚¿ãƒ³ã§è¨˜äº‹ã¸é£›ã¶
                st.link_button("è¨˜äº‹ã‚’èª­ã‚€ ğŸ”—", row['link'])
                
    elif view_mode == "ãƒ‡ãƒ¼ã‚¿ãƒ†ãƒ¼ãƒ–ãƒ«":
        st.dataframe(
            filtered_df,
            column_config={
                "date": "æ—¥æ™‚",
                "category": "ãƒ¡ãƒ‡ã‚£ã‚¢",
                "title": "è¦‹å‡ºã—",
                "summary": "è¦ç´„",
                "link": st.column_config.LinkColumn("ãƒªãƒ³ã‚¯", display_text="è¨˜äº‹ã‚’é–‹ã")
            },
            use_container_width=True,
            hide_index=True
        )
else:
    st.warning("æ¡ä»¶ã«ä¸€è‡´ã™ã‚‹ãƒ‹ãƒ¥ãƒ¼ã‚¹ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")

# --- 4. ãƒ•ãƒƒã‚¿ãƒ¼ ---
st.markdown("---")
st.caption("Powered by Google News RSS")
