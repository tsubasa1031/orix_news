import streamlit as st
import pandas as pd
import requests
from bs4 import BeautifulSoup
import datetime
import time
import difflib
import re
import json

# --- è¿½åŠ : Google Generative AI ãƒ©ã‚¤ãƒ–ãƒ©ãƒª ---
try:
    import google.generativeai as genai
    HAS_GENAI = True
except ImportError:
    HAS_GENAI = False

# ãƒšãƒ¼ã‚¸è¨­å®š
st.set_page_config(
    page_title="ã‚ªãƒªãƒƒã‚¯ã‚¹ãƒ»ãƒãƒ•ã‚¡ãƒ­ãƒ¼ã‚º ãƒ‹ãƒ¥ãƒ¼ã‚¹ã¾ã¨ã‚",
    page_icon="âš¾",
    layout="wide"
)

# --- ã‚«ã‚¹ã‚¿ãƒ CSS: ãƒ‡ã‚¶ã‚¤ãƒ³èª¿æ•´ ---
st.markdown("""
    <style>
    .news-card {
        padding: 1.2rem;
        border-radius: 8px;
        border: 1px solid #e0e0e0;
        margin-bottom: 0.8rem;
        background-color: #ffffff;
        box-shadow: 0 1px 3px rgba(0,0,0,0.05);
        transition: box-shadow 0.2s;
    }
    .news-card:hover {
        box-shadow: 0 4px 6px rgba(0,0,0,0.1);
    }
    .news-header {
        display: flex;
        justify-content: space-between;
        align-items: baseline;
        margin-bottom: 0.5rem;
    }
    .news-category {
        display: inline-block;
        padding: 0.2rem 0.6rem;
        border-radius: 4px;
        font-size: 0.8rem;
        font-weight: bold;
        color: white;
        margin-right: 0.5rem;
    }
    .news-title {
        font-size: 1.15rem;
        font-weight: bold;
        color: #333;
        text-decoration: none;
        display: block;
        margin-bottom: 0.2rem;
    }
    .news-title:hover {
        color: #1f77b4;
        text-decoration: underline;
    }
    .news-meta {
        font-size: 0.8rem;
        color: #777;
    }
    
    /* ã‚«ãƒ†ã‚´ãƒªåˆ¥ã®è‰²å®šç¾© */
    .cat-contract { background-color: #d32f2f; }      /* èµ¤: å¥‘ç´„æ›´æ”¹ */
    .cat-transfer { background-color: #c2185b; }      /* ãƒ”ãƒ³ã‚¯: ç§»ç±/é€€å›£ */
    .cat-draft { background-color: #7b1fa2; }         /* ç´«: ãƒ‰ãƒ©ãƒ•ãƒˆ/æ–°äºº */
    .cat-award { background-color: #fbc02d; color: #333 !important; } /* é‡‘: ã‚¿ã‚¤ãƒˆãƒ« */
    .cat-camp { background-color: #388e3c; }          /* ç·‘: ã‚­ãƒ£ãƒ³ãƒ—/ç·´ç¿’ */
    .cat-game { background-color: #0288d1; }          /* æ°´è‰²: è©¦åˆ */
    .cat-event { background-color: #1976d2; }         /* é’: çƒå›£/ã‚¤ãƒ™ãƒ³ãƒˆ */
    .cat-injury { background-color: #f57c00; }        /* ã‚ªãƒ¬ãƒ³ã‚¸: æ€ªæˆ‘ */
    .cat-other { background-color: #757575; }         /* ã‚°ãƒ¬ãƒ¼ */

    @media (prefers-color-scheme: dark) {
        .news-card {
            background-color: #262730;
            border-color: #444;
        }
        .news-title {
            color: #eee;
        }
        .news-title:hover {
            color: #64b5f6;
        }
        .news-meta {
            color: #aaa;
        }
        .cat-award { color: #000 !important; }
    }
    </style>
    """, unsafe_allow_html=True)

# --- 1. æ¨™æº–æ©Ÿèƒ½ã«ã‚ˆã‚‹ã‚«ãƒ†ã‚´ãƒªåˆ¤å®š (åˆæœŸè¡¨ç¤ºç”¨ãƒ»APIãªã—ç”¨) ---
def assign_category_simple(text):
    text = text.replace(" ", "")
    # ç°¡æ˜“ç‰ˆã‚«ãƒ†ã‚´ãƒªå®šç¾©
    categories = [
        {"name": "ã‚¿ã‚¤ãƒˆãƒ«å—è³", "keywords": ["ãƒ™ã‚¹ãƒˆãƒŠã‚¤ãƒ³", "ã‚´ãƒ¼ãƒ«ãƒ‡ãƒ³ã‚°ãƒ©ãƒ–", "è¡¨å½°", "å—è³", "MVP", "æ–°äººç‹"]},
        {"name": "å¥‘ç´„ãƒ»ç§»ç±", "keywords": ["å¥‘ç´„æ›´æ”¹", "æ›´æ”¹", "ç§»ç±", "FA", "ãƒˆãƒ¬ãƒ¼ãƒ‰", "é€€å›£", "æˆ¦åŠ›å¤–", "ãƒ‰ãƒ©ãƒ•ãƒˆ", "ç²å¾—", "å¹´ä¿¸", "ã‚µã‚¤ãƒ³", "æ®‹ç•™", "ä¸‡å††", "å„„å††"]},
        {"name": "æ€ªæˆ‘ãƒ»èª¿æ•´", "keywords": ["æ€ªæˆ‘", "æ•…éšœ", "æ‰‹è¡“", "é›¢è„±", "ãƒªãƒãƒ“ãƒª", "ç—›", "é•å’Œæ„Ÿ", "ç—…é™¢"]},
        {"name": "ã‚­ãƒ£ãƒ³ãƒ—ãƒ»ç·´ç¿’", "keywords": ["ã‚­ãƒ£ãƒ³ãƒ—", "è‡ªä¸»ãƒˆãƒ¬", "ç·´ç¿’", "ãƒ–ãƒ«ãƒšãƒ³", "å§‹å‹•"]},
        {"name": "çƒå›£ãƒ»ã‚¤ãƒ™ãƒ³ãƒˆ", "keywords": ["ãƒ•ã‚¡ãƒ³æ„Ÿ", "ã‚¤ãƒ™ãƒ³ãƒˆ", "ãƒ¦ãƒ‹ãƒ•ã‚©ãƒ¼ãƒ ", "ãƒ­ã‚´", "ãƒã‚±ãƒƒãƒˆ", "äººäº‹", "ã‚³ãƒ¼ãƒ"]}
    ]
    for cat in categories:
        if any(word in text for word in cat["keywords"]):
            return cat["name"]
    return "ãã®ä»–"

# --- 2. AIã«ã‚ˆã‚‹ä¸€æ‹¬ã‚«ãƒ†ã‚´ãƒªåˆ¤å®š (Gemini API) ---
def categorize_batch_with_ai(news_df, api_key):
    """
    ãƒ‹ãƒ¥ãƒ¼ã‚¹ã®ã‚¿ã‚¤ãƒˆãƒ«ãƒªã‚¹ãƒˆã‚’ä¸€æ‹¬ã§AIã«é€ä¿¡ã—ã€è©³ç´°ãªã‚«ãƒ†ã‚´ãƒªã‚’åˆ¤å®šã•ã›ã‚‹
    """
    if not HAS_GENAI:
        st.error("google-generativeai ãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
        return news_df

    genai.configure(api_key=api_key)
    model = genai.GenerativeModel('gemini-1.5-flash')

    # ã‚¿ã‚¤ãƒˆãƒ«ã¨IDã®ãƒªã‚¹ãƒˆã‚’ä½œæˆ
    titles_data = []
    for idx, row in news_df.iterrows():
        titles_data.append({"id": idx, "title": row['title']})

    # ãƒãƒƒãƒã‚µã‚¤ã‚º (ä¸€åº¦ã«å‡¦ç†ã™ã‚‹ä»¶æ•°)
    BATCH_SIZE = 30
    
    # ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼
    progress_bar = st.progress(0)
    
    # ãƒãƒ£ãƒ³ã‚¯ã”ã¨ã«å‡¦ç†
    for i in range(0, len(titles_data), BATCH_SIZE):
        chunk = titles_data[i:i + BATCH_SIZE]
        
        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆä½œæˆ (å®šç¾©ã‚’å³æ ¼åŒ–)
        prompt = f"""
        ã‚ãªãŸã¯ãƒ—ãƒ­é‡çƒãƒ‹ãƒ¥ãƒ¼ã‚¹ã®ç·¨é›†è€…ã§ã™ã€‚
        ä»¥ä¸‹ã®JSONå½¢å¼ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚¿ã‚¤ãƒˆãƒ«ãƒªã‚¹ãƒˆã‚’èª­ã¿ã€ãã‚Œãã‚Œã®è¨˜äº‹ã‚’æœ€ã‚‚é©åˆ‡ãªã‚«ãƒ†ã‚´ãƒªã«åˆ†é¡ã—ã¦ãã ã•ã„ã€‚
        åˆ¤æ–­ã«è¿·ã†å ´åˆã¯ç„¡ç†ã«åˆ†é¡ã›ãšã€Œãã®ä»–ã€ã‚’é¸æŠã—ã¦ãã ã•ã„ã€‚
        
        ã€ã‚«ãƒ†ã‚´ãƒªå®šç¾©ã¨åˆ¤å®šåŸºæº–ã€‘
        1. å¥‘ç´„æ›´æ”¹
           - å¯¾è±¡: æ—¢å­˜é¸æ‰‹ã®æ¥å­£å¥‘ç´„ã€å¹´ä¿¸äº¤æ¸‰ã€ã‚µã‚¤ãƒ³ã€ç¾çŠ¶ç¶­æŒã€ã‚¢ãƒƒãƒ—ã€ãƒ€ã‚¦ãƒ³ã€‚
           - é™¤å¤–: FAå®£è¨€ã€é€€å›£ã€ç§»ç±ã€æ–°å¤–å›½äººç²å¾—ã¯ã“ã“ã«ã¯å«ã‚ãªã„ã€‚
        
        2. ç§»ç±ãƒ»é€€å›£
           - å¯¾è±¡: FAæ¨©è¡Œä½¿ã€ä»–çƒå›£ã¸ã®ç§»ç±ã€æ–°å¤–å›½äººç²å¾—ã€æˆ¦åŠ›å¤–é€šå‘Šã€è‡ªç”±å¥‘ç´„ã€é€€å›£ã€ãƒã‚¹ãƒ†ã‚£ãƒ³ã‚°ã‚·ã‚¹ãƒ†ãƒ ã€‚
           - é™¤å¤–: ãƒ‰ãƒ©ãƒ•ãƒˆæŒ‡åã¯ã“ã“ã«ã¯å«ã‚ãªã„ã€‚
        
        3. ãƒ‰ãƒ©ãƒ•ãƒˆãƒ»æ–°äºº
           - å¯¾è±¡: ãƒ‰ãƒ©ãƒ•ãƒˆä¼šè­°ã§ã®æŒ‡åã€æŒ‡åæŒ¨æ‹¶ã€ä»®å¥‘ç´„ã€æ–°å…¥å›£é¸æ‰‹ç™ºè¡¨ä¼šè¦‹ã€ãƒ«ãƒ¼ã‚­ãƒ¼ã®ç´¹ä»‹ã€‚
           - é™¤å¤–: æ–°å¤–å›½äººé¸æ‰‹ã¯ã€Œç§»ç±ãƒ»é€€å›£ã€ã¸ã€‚
        
        4. æ€ªæˆ‘ãƒ»èª¿æ•´
           - å¯¾è±¡: æ‰‹è¡“ã€ãƒªãƒãƒ“ãƒªã€æ€ªæˆ‘ã®è¨ºæ–­çµæœã€ç™»éŒ²æŠ¹æ¶ˆï¼ˆæ€ªæˆ‘ç†ç”±ï¼‰ã€ã‚³ãƒ³ãƒ‡ã‚£ã‚·ãƒ§ãƒ³ä¸è‰¯ã€åˆ¥ãƒ¡ãƒ‹ãƒ¥ãƒ¼èª¿æ•´ã€‚
        
        5. ã‚­ãƒ£ãƒ³ãƒ—ãƒ»ç·´ç¿’
           - å¯¾è±¡: æ˜¥å­£/ç§‹å­£ã‚­ãƒ£ãƒ³ãƒ—ã€è‡ªä¸»ãƒˆãƒ¬å…¬é–‹ã€ãƒ–ãƒ«ãƒšãƒ³å…¥ã‚Šã€æ‰“æ’ƒç·´ç¿’ã€ç·´ç¿’è©¦åˆã€ç´…ç™½æˆ¦ã€‚
           - é™¤å¤–: å…¬å¼æˆ¦ã®è©¦åˆçµæœã¯å«ã‚ãªã„ã€‚
        
        6. ã‚¿ã‚¤ãƒˆãƒ«å—è³
           - å¯¾è±¡: ãƒ™ã‚¹ãƒˆãƒŠã‚¤ãƒ³ã€ã‚´ãƒ¼ãƒ«ãƒ‡ãƒ³ã‚°ãƒ©ãƒ–ã€MVPã€æ–°äººç‹ã€æœˆé–“MVPã€å„ç¨®è¡¨å½°ã€‚
        
        7. è©¦åˆãƒ»çµæœ
           - å¯¾è±¡: å…¬å¼æˆ¦ã€äº¤æµæˆ¦ã€CSã€æ—¥æœ¬ã‚·ãƒªãƒ¼ã‚ºã®å‹æ•—ãƒ»ã‚¹ã‚³ã‚¢ãƒ»è©¦åˆçµŒéã€‚
           - é™¤å¤–: ç·´ç¿’è©¦åˆã€ç´…ç™½æˆ¦ã¯ã€Œã‚­ãƒ£ãƒ³ãƒ—ãƒ»ç·´ç¿’ã€ã¸ã€‚
        
        8. çƒå›£ãƒ»ã‚¤ãƒ™ãƒ³ãƒˆ
           - å¯¾è±¡: ãƒ¦ãƒ‹ãƒ•ã‚©ãƒ¼ãƒ ç™ºè¡¨ã€ãƒ­ã‚´å¤‰æ›´ã€ãƒ•ã‚¡ãƒ³æ„Ÿè¬ãƒ‡ãƒ¼ã€ãƒã‚±ãƒƒãƒˆè²©å£²ã€ã‚³ãƒ¼ãƒå°±ä»»ãƒ»é€€ä»»ãªã©ã®äººäº‹ã€ãƒã‚¹ã‚³ãƒƒãƒˆã€ã‚°ãƒƒã‚ºã€‚
        
        9. ãã®ä»–
           - ä¸Šè¨˜ã®ã„ãšã‚Œã«ã‚‚æ˜ç¢ºã«å½“ã¦ã¯ã¾ã‚‰ãªã„ã‚‚ã®ã€‚
        
        ã€å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã€‘
        {json.dumps(chunk, ensure_ascii=False)}
        
        ã€å‡ºåŠ›å½¢å¼ã€‘
        ä»¥ä¸‹ã®JSONãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã®ãƒªã‚¹ãƒˆã®ã¿ã‚’å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚Markdownã®ã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯(```jsonãªã©)ã¯å«ã‚ãªã„ã§ãã ã•ã„ã€‚
        [
            {{"id": 0, "category": "å¥‘ç´„æ›´æ”¹"}},
            {{"id": 1, "category": "æ€ªæˆ‘ãƒ»èª¿æ•´"}}
        ]
        """
        
        try:
            response = model.generate_content(prompt)
            result_text = response.text.strip()
            
            # Markdownã®ã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ãŒã‚ã‚Œã°å‰Šé™¤
            if result_text.startswith("```"):
                result_text = result_text.replace("```json", "").replace("```", "").strip()
            
            # JSONãƒ‘ãƒ¼ã‚¹
            results = json.loads(result_text)
            
            # çµæœã‚’DataFrameã«åæ˜ 
            for res in results:
                idx = res.get("id")
                category = res.get("category")
                if idx is not None and category:
                    # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãŒå­˜åœ¨ã™ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¦æ›´æ–°
                    if idx in news_df.index:
                        news_df.at[idx, 'category'] = category
                        
        except Exception as e:
            print(f"Batch processing failed: {e}")
            # ã‚¨ãƒ©ãƒ¼æ™‚ã¯ã‚¹ã‚­ãƒƒãƒ—ï¼ˆå…ƒã®ã‚«ãƒ†ã‚´ãƒªã®ã¾ã¾ï¼‰
        
        # é€²æ—æ›´æ–°
        progress_bar.progress(min((i + BATCH_SIZE) / len(titles_data), 1.0))
        time.sleep(1) # å®‰å…¨ã®ãŸã‚å°‘ã—å¾…æ©Ÿ

    progress_bar.empty()
    return news_df

def clean_summary(text):
    soup = BeautifulSoup(text, "html.parser")
    text = soup.get_text()
    text = re.sub(r'\s+', ' ', text).strip()
    text = text.replace("è¨˜äº‹ã‚’èª­ã‚€", "").replace("Full coverage", "")
    if len(text) > 100:
        text = text[:100] + "..."
    return text

# --- 3. ãƒ‡ãƒ¼ã‚¿å–å¾—é–¢æ•° ---
@st.cache_data(ttl=1800)
def load_data():
    search_queries = [
        "ã‚ªãƒªãƒƒã‚¯ã‚¹+ãƒãƒ•ã‚¡ãƒ­ãƒ¼ã‚º",
        "ã‚ªãƒªãƒƒã‚¯ã‚¹+å¥‘ç´„æ›´æ”¹",
        "ã‚ªãƒªãƒƒã‚¯ã‚¹+ç§»ç±",
        "ã‚ªãƒªãƒƒã‚¯ã‚¹+ãƒ‰ãƒ©ãƒ•ãƒˆ",
        "ã‚ªãƒªãƒƒã‚¯ã‚¹+ã‚­ãƒ£ãƒ³ãƒ—"
    ]
    
    all_news_list = []
    seen_links = set()
    
    with st.spinner('ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’åé›†ä¸­...'):
        for query in search_queries:
            url = f"https://news.google.com/rss/search?q={query}&hl=ja&gl=JP&ceid=JP:ja"
            
            try:
                response = requests.get(url, timeout=10)
                response.raise_for_status()
                
                soup = BeautifulSoup(response.content, "xml")
                items = soup.find_all("item")
                
                for item in items:
                    title = item.title.text
                    
                    if "ã‚ªãƒªãƒƒã‚¯ã‚¹" not in title and "ãƒãƒ•ã‚¡ãƒ­ãƒ¼ã‚º" not in title and "ä¸­å¶‹" not in title and "å²¸ç”°" not in title:
                         if "Bs" not in title: 
                            continue

                    link = item.link.text
                    if link in seen_links:
                        continue
                    seen_links.add(link)

                    pub_date_str = item.pubDate.text
                    
                    try:
                        timestamp = pd.to_datetime(pub_date_str)
                        if timestamp.tzinfo is None:
                            timestamp = timestamp.tz_localize('UTC')
                        else:
                            timestamp = timestamp.tz_convert('UTC')
                        timestamp_jst = timestamp.tz_convert('Asia/Tokyo')
                        display_date = timestamp_jst.strftime('%m/%d %H:%M')
                    except:
                        timestamp_jst = pd.Timestamp.now(tz='Asia/Tokyo')
                        display_date = pub_date_str

                    source = "News"
                    clean_title = title
                    if " - " in title:
                        parts = title.rsplit(" - ", 1)
                        clean_title = parts[0]
                        source = parts[1]

                    # åˆæœŸã‚«ãƒ†ã‚´ãƒªåˆ¤å®šï¼ˆã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ™ãƒ¼ã‚¹ï¼‰
                    category = assign_category_simple(clean_title)

                    all_news_list.append({
                        "timestamp": timestamp_jst,
                        "date": display_date,
                        "category": category, # å¾Œã§AIã§ä¸Šæ›¸ãå¯èƒ½
                        "media": source,
                        "title": clean_title,
                        "link": link,
                    })
                
                time.sleep(0.5)

            except Exception as e:
                print(f"Query '{query}' failed: {e}")
                continue

    if not all_news_list:
        return pd.DataFrame([{"timestamp": pd.Timestamp.now(), "date": "-", "category": "Error", "media": "-", "title": "ãƒ‡ãƒ¼ã‚¿å–å¾—ã‚¨ãƒ©ãƒ¼", "link": "#"}])

    df = pd.DataFrame(all_news_list)
    df = df.sort_values("timestamp", ascending=False).reset_index(drop=True)
    
    unique_indices = []
    titles = df["title"].tolist()
    for i in range(len(titles)):
        is_duplicate = False
        for j in unique_indices:
            similarity = difflib.SequenceMatcher(None, titles[i], titles[j]).ratio()
            if similarity > 0.6: 
                is_duplicate = True
                break
        if not is_duplicate:
            unique_indices.append(i)
    
    df = df.iloc[unique_indices].reset_index(drop=True)
    return df

# --- ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³çŠ¶æ…‹ã®ç®¡ç† ---
if 'news_df' not in st.session_state:
    st.session_state.news_df = load_data()

# ã‚µã‚¤ãƒ‰ãƒãƒ¼è¨­å®š
st.sidebar.title("ğŸ” è¨­å®šãƒ»æ¤œç´¢")

# APIã‚­ãƒ¼ã‚’å†…éƒ¨ã§ä¿æŒï¼ˆéš è”½ï¼‰
API_KEY = "AIzaSyCc-6JTVoHwkyoT071WBVVXd_F_6I5yA84"
    
sort_order = st.sidebar.radio("ä¸¦ã³é †", ["æ–°ã—ã„é †", "å¤ã„é †"], horizontal=True)

# ãƒ‡ãƒ¼ã‚¿å–å¾—ãƒ»æ›´æ–°
col1, col2 = st.columns([1, 2])
with col1:
    if st.button("ğŸ”„ ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’æ›´æ–°"):
        load_data.clear()
        st.session_state.news_df = load_data()
        st.rerun()

with col2:
    if HAS_GENAI and API_KEY:
        if st.button("âœ¨ AIã§ã‚«ãƒ†ã‚´ãƒªç´°åˆ†åŒ–"):
            if not st.session_state.news_df.empty:
                with st.spinner("AIãŒã‚¿ã‚¤ãƒˆãƒ«ã‚’åˆ†æã—ã¦ã‚«ãƒ†ã‚´ãƒªã‚’æŒ¯ã‚Šåˆ†ã‘ã¦ã„ã¾ã™..."):
                    # ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã”ã¨æ¸¡ã—ã¦æ›´æ–°
                    updated_df = categorize_batch_with_ai(st.session_state.news_df.copy(), API_KEY)
                    st.session_state.news_df = updated_df
                    st.success("ã‚«ãƒ†ã‚´ãƒªã®ç´°åˆ†åŒ–ãŒå®Œäº†ã—ã¾ã—ãŸï¼")
                    st.rerun()
    elif not HAS_GENAI:
        st.error("google-generativeai ãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒå¿…è¦ã§ã™")

# ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ å–å¾—
df = st.session_state.news_df.copy()

# ã‚½ãƒ¼ãƒˆåæ˜ 
if sort_order == "å¤ã„é †":
    df = df.sort_values("timestamp", ascending=True).reset_index(drop=True)
else:
    df = df.sort_values("timestamp", ascending=False).reset_index(drop=True)

# ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
if not df.empty:
    categories = sorted(df["category"].unique())
    if "ãã®ä»–" in categories:
        categories.remove("ãã®ä»–")
        categories.append("ãã®ä»–")

    selected_categories = st.sidebar.multiselect(
        "ã‚«ãƒ†ã‚´ãƒªã§çµã‚Šè¾¼ã¿", categories, default=categories
    )
    
    search_query = st.sidebar.text_input("ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢")
    
    filtered_df = df[df["category"].isin(selected_categories)]
    
    if search_query:
        filtered_df = filtered_df[
            filtered_df["title"].str.contains(search_query, case=False)
        ]
else:
    filtered_df = df

# --- ãƒ¡ã‚¤ãƒ³ç”»é¢ ---
st.title("âš¾ ã‚ªãƒªãƒƒã‚¯ã‚¹ãƒ»ãƒãƒ•ã‚¡ãƒ­ãƒ¼ã‚º æœ€æ–°ãƒ‹ãƒ¥ãƒ¼ã‚¹")
st.caption("æœ€æ–°ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’è‡ªå‹•åé›†ã—ã¦è¡¨ç¤ºã—ã¦ã„ã¾ã™")
st.markdown("---")

if not filtered_df.empty:
    for index, row in filtered_df.iterrows():
        # ã‚«ãƒ†ã‚´ãƒªã«å¿œã˜ãŸCSSã‚¯ãƒ©ã‚¹
        cat_class = "cat-other"
        cat = row['category']
        if "å¥‘ç´„" in cat or "å¹´ä¿¸" in cat: cat_class = "cat-contract"
        elif "ç§»ç±" in cat or "é€€å›£" in cat or "FA" in cat: cat_class = "cat-transfer"
        elif "ãƒ‰ãƒ©ãƒ•ãƒˆ" in cat or "æ–°äºº" in cat: cat_class = "cat-draft"
        elif "ã‚¿ã‚¤ãƒˆãƒ«" in cat or "è¡¨å½°" in cat: cat_class = "cat-award"
        elif "æ€ªæˆ‘" in cat or "èª¿æ•´" in cat: cat_class = "cat-injury"
        elif "çƒå›£" in cat or "ã‚¤ãƒ™ãƒ³ãƒˆ" in cat: cat_class = "cat-event"
        elif "ã‚­ãƒ£ãƒ³ãƒ—" in cat or "ç·´ç¿’" in cat: cat_class = "cat-camp"
        elif "è©¦åˆ" in cat: cat_class = "cat-game"

        link_url = row['link']
        
        st.markdown(f"""
        <div class="news-card">
            <div class="news-header">
                <span class="news-category {cat_class}">{row['category']}</span>
                <span class="news-meta">ğŸ“… {row['date']} | ğŸ¢ {row['media']}</span>
            </div>
            <a href="{link_url}" target="_blank" class="news-title">{row['title']}</a>
        </div>
        """, unsafe_allow_html=True)

else:
    st.warning("æ¡ä»¶ã«ä¸€è‡´ã™ã‚‹ãƒ‹ãƒ¥ãƒ¼ã‚¹ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")

# --- ãƒ•ãƒƒã‚¿ãƒ¼ ---
st.markdown("---")
st.caption("Powered by Google News RSS")
