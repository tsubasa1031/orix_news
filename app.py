import streamlit as st
import pandas as pd
import requests
from bs4 import BeautifulSoup
import datetime
import time
import difflib
import re
import json
from collections import Counter

# --- è¿½åŠ : Google Generative AI ãƒ©ã‚¤ãƒ–ãƒ©ãƒª ---
try:
    import google.generativeai as genai
    HAS_GENAI = True
except ImportError:
    HAS_GENAI = False

# ãƒšãƒ¼ã‚¸è¨­å®š
st.set_page_config(
    page_title="ã‚ªãƒªãƒƒã‚¯ã‚¹ãƒ»ãƒãƒ•ã‚¡ãƒ­ãƒ¼ã‚º ãƒ‹ãƒ¥ãƒ¼ã‚¹ã¾ã¨ã‚",
    page_icon="âš¾",
    layout="wide"
)

# --- ã‚«ã‚¹ã‚¿ãƒ CSS: ãƒ‡ã‚¶ã‚¤ãƒ³èª¿æ•´ ---
st.markdown("""
    <style>
    .news-card {
        padding: 1.2rem;
        border-radius: 8px;
        border: 1px solid #e0e0e0;
        margin-bottom: 0.8rem;
        background-color: #ffffff;
        box-shadow: 0 1px 3px rgba(0,0,0,0.05);
        transition: box-shadow 0.2s;
    }
    .news-card:hover {
        box-shadow: 0 4px 6px rgba(0,0,0,0.1);
    }
    .news-header {
        display: flex;
        justify-content: space-between;
        align-items: flex-start;
        margin-bottom: 0.5rem;
        flex-wrap: wrap;
        gap: 0.5rem;
    }
    .tags-container {
        display: flex;
        flex-wrap: wrap;
        gap: 0.3rem;
    }
    .news-tag {
        display: inline-block;
        padding: 0.15rem 0.5rem;
        border-radius: 12px;
        font-size: 0.75rem;
        font-weight: bold;
        color: #333;
        background-color: #f0f0f0;
        border: 1px solid #ddd;
    }
    /* ç‰¹å®šã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å«ã‚€ã‚¿ã‚°ã®è‰²åˆ†ã‘ */
    .tag-contract { background-color: #ffebee; color: #c62828; border-color: #ef9a9a; }
    .tag-transfer { background-color: #fce4ec; color: #880e4f; border-color: #f48fb1; }
    .tag-draft { background-color: #f3e5f5; color: #4a148c; border-color: #ce93d8; }
    .tag-game { background-color: #e1f5fe; color: #01579b; border-color: #81d4fa; }
    .tag-camp { background-color: #e8f5e9; color: #1b5e20; border-color: #a5d6a7; }
    .tag-award { background-color: #fffde7; color: #f57f17; border-color: #fff59d; }
    .tag-injury { background-color: #fff3e0; color: #e65100; border-color: #ffcc80; }
    
    .news-title {
        font-size: 1.15rem;
        font-weight: bold;
        color: #333;
        text-decoration: none;
        display: block;
        margin-top: 0.3rem;
        line-height: 1.4;
    }
    .news-title:hover {
        color: #1f77b4;
        text-decoration: underline;
    }
    .news-meta {
        font-size: 0.8rem;
        color: #777;
        margin-top: 0.3rem;
    }

    @media (prefers-color-scheme: dark) {
        .news-card {
            background-color: #262730;
            border-color: #444;
        }
        .news-title { color: #eee; }
        .news-title:hover { color: #64b5f6; }
        .news-meta { color: #aaa; }
        .news-tag { background-color: #444; color: #ddd; border-color: #555; }
        /* ãƒ€ãƒ¼ã‚¯ãƒ¢ãƒ¼ãƒ‰æ™‚ã®ã‚¿ã‚°è‰²ï¼ˆå°‘ã—æš—ã‚ã«ï¼‰ */
        .tag-contract { background-color: #5c1b1b; color: #ffcdd2; border-color: #ef5350; }
        .tag-transfer { background-color: #4a1428; color: #f8bbd0; border-color: #ec407a; }
        .tag-draft { background-color: #3a1c42; color: #e1bee7; border-color: #ab47bc; }
        .tag-game { background-color: #1a3b4d; color: #b3e5fc; border-color: #29b6f6; }
        .tag-camp { background-color: #1b3e20; color: #c8e6c9; border-color: #66bb6a; }
        .tag-award { background-color: #4a3b0a; color: #fff9c4; border-color: #ffee58; }
        .tag-injury { background-color: #4e2c0c; color: #ffe0b2; border-color: #ffa726; }
    }
    </style>
    """, unsafe_allow_html=True)

# --- 1. æ¨™æº–æ©Ÿèƒ½ã«ã‚ˆã‚‹ã‚¿ã‚°ç”Ÿæˆ (APIãªã—ç”¨) ---
def generate_tags_simple(text):
    text_clean = text.replace(" ", "")
    tags = []
    
    # ã‚«ãƒ†ã‚´ãƒªåˆ¤å®šãƒ­ã‚¸ãƒƒã‚¯
    categories = [
        {"name": "å¥‘ç´„æ›´æ”¹", "keywords": ["å¥‘ç´„æ›´æ”¹", "æ›´æ”¹", "å¹´ä¿¸", "ã‚µã‚¤ãƒ³", "ç¾çŠ¶ç¶­æŒ", "ã‚¢ãƒƒãƒ—", "ãƒ€ã‚¦ãƒ³", "ä¿ç•™"]},
        {"name": "ç§»ç±ãƒ»é€€å›£", "keywords": ["ç§»ç±", "FA", "ãƒˆãƒ¬ãƒ¼ãƒ‰", "é€€å›£", "æˆ¦åŠ›å¤–", "è‡ªç”±å¥‘ç´„", "ãƒã‚¹ãƒ†ã‚£ãƒ³ã‚°", "æ–°å¤–å›½äºº"]},
        {"name": "ãƒ‰ãƒ©ãƒ•ãƒˆãƒ»æ–°äºº", "keywords": ["ãƒ‰ãƒ©ãƒ•ãƒˆ", "æŒ‡å", "å…¥å›£", "æ–°äºº", "ãƒ«ãƒ¼ã‚­ãƒ¼"]},
        {"name": "æ€ªæˆ‘ãƒ»èª¿æ•´", "keywords": ["æ€ªæˆ‘", "æ•…éšœ", "æ‰‹è¡“", "é›¢è„±", "ãƒªãƒãƒ“ãƒª", "ç—›", "é•å’Œæ„Ÿ", "ç—…é™¢", "æŠ¹æ¶ˆ"]},
        {"name": "ã‚­ãƒ£ãƒ³ãƒ—ãƒ»ç·´ç¿’", "keywords": ["ã‚­ãƒ£ãƒ³ãƒ—", "è‡ªä¸»ãƒˆãƒ¬", "ç·´ç¿’", "ãƒ–ãƒ«ãƒšãƒ³", "å§‹å‹•", "ç´…ç™½æˆ¦"]},
        {"name": "ã‚¿ã‚¤ãƒˆãƒ«å—è³", "keywords": ["ãƒ™ã‚¹ãƒˆãƒŠã‚¤ãƒ³", "ã‚´ãƒ¼ãƒ«ãƒ‡ãƒ³ã‚°ãƒ©ãƒ–", "è¡¨å½°", "å—è³", "MVP", "æ–°äººç‹"]},
        {"name": "çƒå›£æƒ…å ±", "keywords": ["ãƒ•ã‚¡ãƒ³æ„Ÿ", "ã‚¤ãƒ™ãƒ³ãƒˆ", "ãƒ¦ãƒ‹ãƒ•ã‚©ãƒ¼ãƒ ", "ãƒ­ã‚´", "ãƒã‚±ãƒƒãƒˆ", "äººäº‹", "ã‚³ãƒ¼ãƒ"]}
    ]
    
    for cat in categories:
        if any(word in text_clean for word in cat["keywords"]):
            tags.append(cat["name"])
            
    # ç°¡æ˜“çš„ãªé¸æ‰‹åæŠ½å‡º (ä»£è¡¨çš„ãªé¸æ‰‹ã®ã¿)
    famous_players = ["ä¸­å¶‹", "å²¸ç”°", "å®®åŸ", "ç´…æ—", "å±±ä¸‹", "å‰ç”°", "æ£®å‹å“‰", "è‹¥æœˆ", "é “å®®", "æ‰æœ¬", "å¹³é‡", "å±±å´", "å®‡ç”°å·", "æ±", "æ›½è°·"]
    for player in famous_players:
        if player in text_clean:
            tags.append(player)
            
    if not tags:
        tags.append("ãƒ‹ãƒ¥ãƒ¼ã‚¹")
        
    return list(set(tags)) # é‡è¤‡æ’é™¤

# --- 2. AIã«ã‚ˆã‚‹ã‚¿ã‚°ç”Ÿæˆ (Gemini API) ---
def tag_batch_with_ai(news_df, api_key):
    """
    ãƒ‹ãƒ¥ãƒ¼ã‚¹ã®ã‚¿ã‚¤ãƒˆãƒ«ãƒªã‚¹ãƒˆã‚’ä¸€æ‹¬ã§AIã«é€ä¿¡ã—ã€é©åˆ‡ãªã‚¿ã‚°ï¼ˆé¸æ‰‹åã€ãƒˆãƒ”ãƒƒã‚¯ãªã©ï¼‰ã‚’ç”Ÿæˆã•ã›ã‚‹
    """
    if not HAS_GENAI:
        st.error("google-generativeai ãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
        return news_df

    genai.configure(api_key=api_key)
    model = genai.GenerativeModel('gemini-1.5-flash')

    titles_data = []
    for idx, row in news_df.iterrows():
        titles_data.append({"id": idx, "title": row['title']})

    BATCH_SIZE = 20 # å‡¦ç†é€Ÿåº¦ã¨ç²¾åº¦ã®ãƒãƒ©ãƒ³ã‚¹
    
    progress_bar = st.progress(0)
    
    for i in range(0, len(titles_data), BATCH_SIZE):
        chunk = titles_data[i:i + BATCH_SIZE]
        
        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ: ã‚¿ã‚¤ãƒˆãƒ«ã‹ã‚‰è¤‡æ•°ã®ã‚¿ã‚°ã‚’æŠ½å‡ºã•ã›ã‚‹
        prompt = f"""
        ã‚ãªãŸã¯ãƒ—ãƒ­é‡çƒãƒ‹ãƒ¥ãƒ¼ã‚¹ã®ã‚¿ã‚°ä»˜ã‘æ‹…å½“ã§ã™ã€‚
        ä»¥ä¸‹ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚¿ã‚¤ãƒˆãƒ«ã®ãƒªã‚¹ãƒˆã‹ã‚‰ã€ãã‚Œãã‚Œã®è¨˜äº‹ã«é©ã—ãŸã‚¿ã‚°ï¼ˆã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼‰ã‚’æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚
        
        ã€æŠ½å‡ºãƒ«ãƒ¼ãƒ«ã€‘
        1. **ãƒˆãƒ”ãƒƒã‚¯ã‚¿ã‚°**: ä»¥ä¸‹ã®ãƒªã‚¹ãƒˆã‹ã‚‰è©²å½“ã™ã‚‹ã‚‚ã®ã‚’1ã¤ä»¥ä¸Šé¸ã‚“ã§ãã ã•ã„ã€‚
           - å¥‘ç´„æ›´æ”¹, ç§»ç±ãƒ»é€€å›£, ãƒ‰ãƒ©ãƒ•ãƒˆãƒ»æ–°äºº, æ€ªæˆ‘ãƒ»èª¿æ•´, ã‚­ãƒ£ãƒ³ãƒ—ãƒ»ç·´ç¿’, ã‚¿ã‚¤ãƒˆãƒ«å—è³, è©¦åˆãƒ»çµæœ, çƒå›£æƒ…å ±
        2. **ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã‚¿ã‚°**: è¨˜äº‹ã«ç™»å ´ã™ã‚‹å…·ä½“çš„ãªã€Œé¸æ‰‹åã€ã€Œç›£ç£åã€ã€Œç›¸æ‰‹çƒå›£åã€ã‚’æŠ½å‡ºã—ã¦ãã ã•ã„ï¼ˆä¾‹: å®®åŸå¤§å¼¥, å²¸ç”°ç›£ç£, é˜ªç¥ï¼‰ã€‚
        3. **è©³ç´°ã‚¿ã‚°**: å…·ä½“çš„ãªå†…å®¹ãŒã‚ã‚Œã°çŸ­ãæŠ½å‡ºã—ã¦ãã ã•ã„ï¼ˆä¾‹: 1000ä¸‡å¢—, é›¢è„±ï¼‰ã€‚
        
        ã€å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ (JSON)ã€‘
        {json.dumps(chunk, ensure_ascii=False)}
        
        ã€å‡ºåŠ›å½¢å¼ã€‘
        ä»¥ä¸‹ã®JSONãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã®ãƒªã‚¹ãƒˆã®ã¿ã‚’å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚Markdownã‚¿ã‚°ã¯ä¸è¦ã§ã™ã€‚
        [
            {{"id": 0, "tags": ["å®®åŸå¤§å¼¥", "å¥‘ç´„æ›´æ”¹", "1å„„è¶…ãˆ"]}},
            {{"id": 1, "tags": ["å±±ä¸‹èˆœå¹³å¤§", "æ€ªæˆ‘ãƒ»èª¿æ•´", "è…°ç—›"]}}
        ]
        """
        
        try:
            response = model.generate_content(prompt)
            result_text = response.text.strip()
            
            if result_text.startswith("```"):
                result_text = result_text.replace("```json", "").replace("```", "").strip()
            
            results = json.loads(result_text)
            
            for res in results:
                idx = res.get("id")
                tags = res.get("tags")
                if idx is not None and tags and isinstance(tags, list):
                    if idx in news_df.index:
                        # æ—¢å­˜ã®ã‚¿ã‚°ã‚’AIç”Ÿæˆã‚¿ã‚°ã§ä¸Šæ›¸ã
                        news_df.at[idx, 'tags'] = tags
                        
        except Exception as e:
            print(f"Batch processing failed: {e}")
        
        progress_bar.progress(min((i + BATCH_SIZE) / len(titles_data), 1.0))
        time.sleep(1)

    progress_bar.empty()
    return news_df

# --- 3. ãƒ‡ãƒ¼ã‚¿å–å¾—é–¢æ•° ---
@st.cache_data(ttl=1800)
def load_data():
    search_queries = [
        "ã‚ªãƒªãƒƒã‚¯ã‚¹+ãƒãƒ•ã‚¡ãƒ­ãƒ¼ã‚º",
        "ã‚ªãƒªãƒƒã‚¯ã‚¹+å¥‘ç´„æ›´æ”¹",
        "ã‚ªãƒªãƒƒã‚¯ã‚¹+ç§»ç±",
        "ã‚ªãƒªãƒƒã‚¯ã‚¹+ãƒ‰ãƒ©ãƒ•ãƒˆ",
        "ã‚ªãƒªãƒƒã‚¯ã‚¹+ã‚­ãƒ£ãƒ³ãƒ—"
    ]
    
    all_news_list = []
    seen_links = set()
    
    # User-Agentã‚’è¨­å®šã—ã¦ãƒ–ãƒ©ã‚¦ã‚¶ã‹ã‚‰ã®ã‚¢ã‚¯ã‚»ã‚¹ã«è¦‹ã›ã‚‹
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
    }
    
    with st.spinner('ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’åé›†ä¸­...'):
        for query in search_queries:
            # URLã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°å¯¾ç­–ã¨ã—ã¦ requests ã® params ã‚’ä½¿ç”¨ã™ã‚‹ã‹ã€
            # ç¢ºå®Ÿã«å‹•ä½œã•ã›ã‚‹ãŸã‚ã«æ–‡å­—åˆ—æ§‹ç¯‰æ™‚ã«æ³¨æ„ã™ã‚‹ã€‚
            # ã“ã“ã§ã¯ã‚·ãƒ³ãƒ—ãƒ«ã«f-stringã§æ§‹ç¯‰ã™ã‚‹ãŒã€User-AgentãŒé‡è¦ã€‚
            url = f"[https://news.google.com/rss/search?q=](https://news.google.com/rss/search?q=){query}&hl=ja&gl=JP&ceid=JP:ja"
            
            try:
                # headersã‚’è¿½åŠ 
                response = requests.get(url, headers=headers, timeout=10)
                response.raise_for_status()
                
                soup = BeautifulSoup(response.content, "xml")
                items = soup.find_all("item")
                
                for item in items:
                    title = item.title.text
                    
                    if "ã‚ªãƒªãƒƒã‚¯ã‚¹" not in title and "ãƒãƒ•ã‚¡ãƒ­ãƒ¼ã‚º" not in title and "ä¸­å¶‹" not in title and "å²¸ç”°" not in title:
                         if "Bs" not in title: 
                            continue

                    link = item.link.text
                    if link in seen_links:
                        continue
                    seen_links.add(link)

                    pub_date_str = item.pubDate.text
                    try:
                        timestamp = pd.to_datetime(pub_date_str)
                        if timestamp.tzinfo is None:
                            timestamp = timestamp.tz_localize('UTC')
                        else:
                            timestamp = timestamp.tz_convert('UTC')
                        timestamp_jst = timestamp.tz_convert('Asia/Tokyo')
                        display_date = timestamp_jst.strftime('%m/%d %H:%M')
                    except:
                        timestamp_jst = pd.Timestamp.now(tz='Asia/Tokyo')
                        display_date = pub_date_str

                    source = "News"
                    clean_title = title
                    if " - " in title:
                        parts = title.rsplit(" - ", 1)
                        clean_title = parts[0]
                        source = parts[1]

                    # åˆæœŸã‚¿ã‚°ç”Ÿæˆï¼ˆã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ™ãƒ¼ã‚¹ï¼‰
                    tags = generate_tags_simple(clean_title)

                    all_news_list.append({
                        "timestamp": timestamp_jst,
                        "date": display_date,
                        "tags": tags, # ãƒªã‚¹ãƒˆå½¢å¼
                        "media": source,
                        "title": clean_title,
                        "link": link,
                    })
                
                time.sleep(0.5)

            except Exception as e:
                print(f"Query '{query}' failed: {e}")
                # st.error(f"ã‚¨ãƒ©ãƒ¼: {e}") # UIãŒå´©ã‚Œã‚‹ã®ã§ãƒ­ã‚°ã®ã¿
                continue

    if not all_news_list:
        return pd.DataFrame([{"timestamp": pd.Timestamp.now(), "date": "-", "tags": ["Error"], "media": "System", "title": "ãƒ‡ãƒ¼ã‚¿å–å¾—ã‚¨ãƒ©ãƒ¼: å†èª­ã¿è¾¼ã¿ã—ã¦ãã ã•ã„", "link": "#"}])

    df = pd.DataFrame(all_news_list)
    df = df.sort_values("timestamp", ascending=False).reset_index(drop=True)
    
    unique_indices = []
    titles = df["title"].tolist()
    for i in range(len(titles)):
        is_duplicate = False
        for j in unique_indices:
            similarity = difflib.SequenceMatcher(None, titles[i], titles[j]).ratio()
            if similarity > 0.6: 
                is_duplicate = True
                break
        if not is_duplicate:
            unique_indices.append(i)
    
    df = df.iloc[unique_indices].reset_index(drop=True)
    return df

# --- ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³çŠ¶æ…‹ã®ç®¡ç† ---
# ãƒ‡ãƒ¼ã‚¿ã®å†èª­ã¿è¾¼ã¿ãƒã‚§ãƒƒã‚¯: 'tags' ã‚«ãƒ©ãƒ ãŒãªã„å ´åˆï¼ˆå¤ã„ãƒ‡ãƒ¼ã‚¿å½¢å¼ï¼‰ã¯å¼·åˆ¶çš„ã«ãƒªãƒ­ãƒ¼ãƒ‰
if 'news_df' not in st.session_state or 'tags' not in st.session_state.news_df.columns:
    st.session_state.news_df = load_data()

# ã‚µã‚¤ãƒ‰ãƒãƒ¼è¨­å®š
st.sidebar.title("ğŸ” è¨­å®šãƒ»æ¤œç´¢")

# APIã‚­ãƒ¼ã‚’å†…éƒ¨ã§ä¿æŒ
API_KEY = "AIzaSyCc-6JTVoHwkyoT071WBVVXd_F_6I5yA84"
    
sort_order = st.sidebar.radio("ä¸¦ã³é †", ["æ–°ã—ã„é †", "å¤ã„é †"], horizontal=True)

# ãƒ‡ãƒ¼ã‚¿å–å¾—ãƒ»æ›´æ–°
col1, col2 = st.columns([1, 2])
with col1:
    if st.button("ğŸ”„ ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’æ›´æ–°"):
        load_data.clear()
        st.session_state.news_df = load_data()
        st.rerun()

with col2:
    if HAS_GENAI and API_KEY:
        if st.button("âœ¨ AIã§ã‚¿ã‚°ä»˜ã‘è©³ç´°åŒ–"):
            if not st.session_state.news_df.empty:
                # ã‚¨ãƒ©ãƒ¼è¡ŒãŒå«ã¾ã‚Œã¦ã„ã‚‹å ´åˆã¯å®Ÿè¡Œã—ãªã„
                if "Error" in st.session_state.news_df.iloc[0]["tags"]:
                     st.error("æœ‰åŠ¹ãªãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
                else:
                    with st.spinner("AIãŒã‚¿ã‚¤ãƒˆãƒ«ã‚’åˆ†æã—ã¦è©³ç´°ãªã‚¿ã‚°ã‚’ç”Ÿæˆä¸­..."):
                        updated_df = tag_batch_with_ai(st.session_state.news_df.copy(), API_KEY)
                        st.session_state.news_df = updated_df
                        st.success("ã‚¿ã‚°ã®ç”ŸæˆãŒå®Œäº†ã—ã¾ã—ãŸï¼")
                        st.rerun()
    elif not HAS_GENAI:
        st.error("google-generativeai ãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒå¿…è¦ã§ã™")

# ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ å–å¾—
df = st.session_state.news_df.copy()

# ã‚½ãƒ¼ãƒˆåæ˜ 
if sort_order == "å¤ã„é †":
    df = df.sort_values("timestamp", ascending=True).reset_index(drop=True)
else:
    df = df.sort_values("timestamp", ascending=False).reset_index(drop=True)

# ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
if not df.empty:
    # å…¨ã‚¿ã‚°ã®ãƒªã‚¹ãƒˆã‚’ä½œæˆ
    all_tags = []
    # ã“ã“ã§ã‚¨ãƒ©ãƒ¼ã«ãªã‚‹ã®ã‚’é˜²ããŸã‚ã€tagsã‚«ãƒ©ãƒ ãŒå­˜åœ¨ã™ã‚‹ã“ã¨ã‚’ä¿è¨¼æ¸ˆã¿
    for tags in df['tags']:
        if isinstance(tags, list):
            all_tags.extend(tags)
        else:
            # ä¸‡ãŒä¸€ãƒªã‚¹ãƒˆã§ãªã„å ´åˆï¼ˆå¤ã„ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãªã©ï¼‰ã®å®‰å…¨ç­–
            all_tags.append(str(tags))
    
    # å‡ºç¾å›æ•°é †ã«ã‚½ãƒ¼ãƒˆã—ã¦ãƒ¦ãƒ‹ãƒ¼ã‚¯åŒ–
    tag_counts = Counter(all_tags)
    sorted_tags = [tag for tag, count in tag_counts.most_common()]

    selected_tags = st.sidebar.multiselect(
        "ã‚¿ã‚°ã§çµã‚Šè¾¼ã¿ (é¸æ‰‹åã€ãƒˆãƒ”ãƒƒã‚¯ãªã©)", sorted_tags
    )
    
    search_query = st.sidebar.text_input("ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢")
    
    # ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ãƒ­ã‚¸ãƒƒã‚¯
    if selected_tags:
        # é¸æŠã•ã‚ŒãŸã‚¿ã‚°ã®ã„ãšã‚Œã‹ã‚’å«ã‚“ã§ã„ã‚‹è¡Œã‚’æŠ½å‡º
        df = df[df['tags'].apply(lambda x: any(tag in x for tag in selected_tags) if isinstance(x, list) else False)]
    
    if search_query:
        df = df[df["title"].str.contains(search_query, case=False)]
else:
    pass

# --- ãƒ¡ã‚¤ãƒ³ç”»é¢ ---
st.title("âš¾ ã‚ªãƒªãƒƒã‚¯ã‚¹ãƒ»ãƒãƒ•ã‚¡ãƒ­ãƒ¼ã‚º æœ€æ–°ãƒ‹ãƒ¥ãƒ¼ã‚¹")
st.caption("æœ€æ–°ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’è‡ªå‹•åé›†ã—ã¦è¡¨ç¤ºã—ã¦ã„ã¾ã™")
st.markdown("---")

if not df.empty:
    for index, row in df.iterrows():
        tags = row['tags'] if isinstance(row['tags'], list) else []
        link_url = row['link']
        
        # ã‚¿ã‚°ã®HTMLç”Ÿæˆ
        tags_html = ""
        for tag in tags:
            # ã‚¿ã‚°ã®å†…å®¹ã«å¿œã˜ã¦ã‚¯ãƒ©ã‚¹ã‚’ä»˜ä¸
            tag_class = ""
            if "å¥‘ç´„" in tag or "æ›´æ”¹" in tag: tag_class = "tag-contract"
            elif "ç§»ç±" in tag or "é€€å›£" in tag: tag_class = "tag-transfer"
            elif "ãƒ‰ãƒ©ãƒ•ãƒˆ" in tag or "æ–°äºº" in tag: tag_class = "tag-draft"
            elif "æ€ªæˆ‘" in tag or "æ‰‹è¡“" in tag: tag_class = "tag-injury"
            elif "ã‚­ãƒ£ãƒ³ãƒ—" in tag or "ç·´ç¿’" in tag: tag_class = "tag-camp"
            elif "ã‚¿ã‚¤ãƒˆãƒ«" in tag or "è³" in tag: tag_class = "tag-award"
            elif "è©¦åˆ" in tag or "å‹" in tag or "è² " in tag: tag_class = "tag-game"
            
            tags_html += f'<span class="news-tag {tag_class}">{tag}</span>'

        st.markdown(f"""
        <div class="news-card">
            <div class="news-header">
                <div class="tags-container">
                    {tags_html}
                </div>
                <span class="news-meta">ğŸ“… {row['date']} | ğŸ¢ {row['media']}</span>
            </div>
            <a href="{link_url}" target="_blank" class="news-title">{row['title']}</a>
        </div>
        """, unsafe_allow_html=True)

else:
    st.warning("æ¡ä»¶ã«ä¸€è‡´ã™ã‚‹ãƒ‹ãƒ¥ãƒ¼ã‚¹ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")

# --- ãƒ•ãƒƒã‚¿ãƒ¼ ---
st.markdown("---")
st.caption("Powered by Google News RSS")
