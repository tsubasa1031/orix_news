import streamlit as st
import pandas as pd
import requests
from bs4 import BeautifulSoup
import datetime
import time
import difflib
import re

# ãƒšãƒ¼ã‚¸è¨­å®š
st.set_page_config(
    page_title="ã‚ªãƒªãƒƒã‚¯ã‚¹ãƒ»ãƒãƒ•ã‚¡ãƒ­ãƒ¼ã‚º ãƒ‹ãƒ¥ãƒ¼ã‚¹ã¾ã¨ã‚",
    page_icon="âš¾",
    layout="wide"
)

# --- ã‚«ã‚¹ã‚¿ãƒ CSS: ãƒ‡ã‚¶ã‚¤ãƒ³èª¿æ•´ ---
st.markdown("""
    <style>
    .news-card {
        padding: 1.2rem;
        border-radius: 8px;
        border: 1px solid #e0e0e0;
        margin-bottom: 0.8rem;
        background-color: #ffffff;
        box-shadow: 0 1px 3px rgba(0,0,0,0.05);
        transition: box-shadow 0.2s;
    }
    .news-card:hover {
        box-shadow: 0 4px 6px rgba(0,0,0,0.1);
    }
    .news-header {
        display: flex;
        justify-content: space-between;
        align-items: baseline;
        margin-bottom: 0.5rem;
    }
    .news-category {
        display: inline-block;
        padding: 0.2rem 0.6rem;
        border-radius: 4px;
        font-size: 0.8rem;
        font-weight: bold;
        color: white;
        margin-right: 0.5rem;
    }
    .news-title {
        font-size: 1.15rem;
        font-weight: bold;
        color: #333;
        text-decoration: none;
        display: block;
        margin-bottom: 0.5rem;
    }
    .news-title:hover {
        color: #1f77b4;
        text-decoration: underline;
    }
    .news-meta {
        font-size: 0.8rem;
        color: #777;
    }
    .news-summary {
        font-size: 0.9rem;
        color: #444;
        line-height: 1.6;
        margin-top: 0.5rem;
    }
    
    /* ã‚«ãƒ†ã‚´ãƒªåˆ¥ã®è‰²å®šç¾© */
    .cat-contract { background-color: #d32f2f; } /* èµ¤: å¥‘ç´„ */
    .cat-camp { background-color: #388e3c; }     /* ç·‘: ã‚­ãƒ£ãƒ³ãƒ—/ç·´ç¿’ */
    .cat-event { background-color: #1976d2; }    /* é’: çƒå›£/ã‚¤ãƒ™ãƒ³ãƒˆ */
    .cat-injury { background-color: #f57c00; }   /* ã‚ªãƒ¬ãƒ³ã‚¸: æ€ªæˆ‘ */
    .cat-other { background-color: #757575; }    /* ã‚°ãƒ¬ãƒ¼ */

    @media (prefers-color-scheme: dark) {
        .news-card {
            background-color: #262730;
            border-color: #444;
        }
        .news-title {
            color: #eee;
        }
        .news-title:hover {
            color: #64b5f6;
        }
        .news-meta {
            color: #aaa;
        }
        .news-summary {
            color: #ccc;
        }
    }
    </style>
    """, unsafe_allow_html=True)

# --- ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•°: ã‚«ãƒ†ã‚´ãƒªåˆ¤å®šã®ç²¾åº¦å‘ä¸Š ---
def assign_category(text):
    """
    ãƒ‹ãƒ¥ãƒ¼ã‚¹ã®ã‚¿ã‚¤ãƒˆãƒ«ãƒ»æœ¬æ–‡ã‹ã‚‰ã‚«ãƒ†ã‚´ãƒªã‚’åˆ¤å®šã™ã‚‹ã€‚
    å„ªå…ˆé †ä½ã‚’è¨­å®šã—ã¦ã€ã‚ˆã‚Šæ­£ç¢ºã«åˆ†é¡ã™ã‚‹ã€‚
    """
    text = text.replace(" ", "")
    
    # ã‚«ãƒ†ã‚´ãƒªå®šç¾© (ä¸Šã«ã‚ã‚‹ã‚‚ã®ã»ã©å„ªå…ˆåº¦ãŒé«˜ã„)
    categories = [
        {
            "name": "å¥‘ç´„ãƒ»ç§»ç±",
            "keywords": ["å¥‘ç´„æ›´æ”¹", "æ›´æ”¹", "ç§»ç±", "FA", "ãƒˆãƒ¬ãƒ¼ãƒ‰", "æ–°åŠ å…¥", "é€€å›£", "æˆ¦åŠ›å¤–", "ãƒ‰ãƒ©ãƒ•ãƒˆ", "ç²å¾—", "ãƒã‚¹ãƒ†ã‚£ãƒ³ã‚°", "è‚²æˆ", "æ”¯é…ä¸‹", "å¹´ä¿¸", "äººçš„è£œå„Ÿ", "å…¥å›£", "ã‚µã‚¤ãƒ³", "æ®‹ç•™"]
        },
        {
            "name": "æ€ªæˆ‘ãƒ»èª¿æ•´",
            "keywords": ["æ€ªæˆ‘", "æ•…éšœ", "æ‰‹è¡“", "é›¢è„±", "å…¨æ²»", "ãƒªãƒãƒ“ãƒª", "ç—›", "é•å’Œæ„Ÿ", "ã‚³ãƒ³ãƒ‡ã‚£ã‚·ãƒ§ãƒ³", "ç—…é™¢", "æ¤œæŸ»"]
        },
        {
            "name": "ã‚­ãƒ£ãƒ³ãƒ—ãƒ»ç·´ç¿’", # ã‚ªãƒ•ã‚·ãƒ¼ã‚ºãƒ³å‘ã‘ã«å¤‰æ›´
            "keywords": ["ã‚­ãƒ£ãƒ³ãƒ—", "è‡ªä¸»ãƒˆãƒ¬", "ç·´ç¿’", "ãƒ–ãƒ«ãƒšãƒ³", "æŠ•ã’è¾¼ã¿", "æ‰“æ’ƒ", "ãƒãƒƒã‚¯", "ç´…ç™½æˆ¦", "ãƒ•ã‚§ãƒ‹ãƒƒã‚¯ã‚¹", "ç§‹å­£", "æ˜¥å­£", "å§‹å‹•"]
        },
        {
            "name": "çƒå›£ãƒ»ã‚¤ãƒ™ãƒ³ãƒˆ",
            "keywords": ["ãƒ•ã‚¡ãƒ³æ„Ÿ", "ã‚¤ãƒ™ãƒ³ãƒˆ", "ãƒ¦ãƒ‹ãƒ•ã‚©ãƒ¼ãƒ ", "ãƒ­ã‚´", "ãƒã‚±ãƒƒãƒˆ", "ã‚°ãƒƒã‚º", "ã‚¹ãƒãƒ³ã‚µãƒ¼", "ãƒã‚¹ã‚³ãƒƒãƒˆ", "äººäº‹", "ã‚³ãƒ¼ãƒ", "ç›£ç£", "ãƒ™ã‚¹ãƒˆãƒŠã‚¤ãƒ³", "ã‚´ãƒ¼ãƒ«ãƒ‡ãƒ³ã‚°ãƒ©ãƒ–", "è¡¨å½°", "ãƒ‘ãƒ¬ãƒ¼ãƒ‰"]
        }
    ]
    
    for cat in categories:
        if any(word in text for word in cat["keywords"]):
            return cat["name"]
            
    return "ãã®ä»–ãƒ‹ãƒ¥ãƒ¼ã‚¹"

def clean_summary(text):
    """
    RSSã®descriptionã‹ã‚‰ä½™è¨ˆãªHTMLã‚¿ã‚°ã‚„ã‚´ãƒŸæ–‡å­—ã‚’é™¤å»ã™ã‚‹
    """
    # HTMLã‚¿ã‚°ã®é™¤å»
    soup = BeautifulSoup(text, "html.parser")
    text = soup.get_text()
    
    # é€£ç¶šã™ã‚‹ç©ºç™½ã‚’1ã¤ã«
    text = re.sub(r'\s+', ' ', text).strip()
    
    # Google Newsç‰¹æœ‰ã®ã€Œè¨˜äº‹ã‚’èª­ã‚€ã€ãªã©ã®ãƒªãƒ³ã‚¯æ–‡å­—ã‚’å‰Šé™¤
    text = text.replace("è¨˜äº‹ã‚’èª­ã‚€", "").replace("Full coverage", "")
    
    # æ–‡æœ«ã®èª¿æ•´
    if len(text) > 100:
        text = text[:100] + "..."
        
    return text

# --- 1. ãƒ‡ãƒ¼ã‚¿å–å¾—é–¢æ•° ---
@st.cache_data(ttl=1800)
def load_data():
    search_queries = [
        "ã‚ªãƒªãƒƒã‚¯ã‚¹+ãƒãƒ•ã‚¡ãƒ­ãƒ¼ã‚º",
        "ã‚ªãƒªãƒƒã‚¯ã‚¹+å¥‘ç´„æ›´æ”¹",
        "ã‚ªãƒªãƒƒã‚¯ã‚¹+ç§»ç±",
        "ã‚ªãƒªãƒƒã‚¯ã‚¹+ãƒ‰ãƒ©ãƒ•ãƒˆ",
        "ã‚ªãƒªãƒƒã‚¯ã‚¹+ã‚­ãƒ£ãƒ³ãƒ—"
    ]
    
    all_news_list = []
    seen_links = set()
    
    with st.spinner('ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’åé›†ä¸­...'):
        for query in search_queries:
            url = f"https://news.google.com/rss/search?q={query}&hl=ja&gl=JP&ceid=JP:ja"
            
            try:
                response = requests.get(url, timeout=10)
                response.raise_for_status()
                
                soup = BeautifulSoup(response.content, "xml")
                items = soup.find_all("item")
                
                for item in items:
                    title = item.title.text
                    
                    # ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
                    if "ã‚ªãƒªãƒƒã‚¯ã‚¹" not in title and "ãƒãƒ•ã‚¡ãƒ­ãƒ¼ã‚º" not in title and "ä¸­å¶‹" not in title and "å²¸ç”°" not in title:
                         # ç›£ç£åãªã©ãŒå«ã¾ã‚Œã¦ã„ã‚Œã°é€šã™ã€ãã‚Œä»¥å¤–ã¯å³ã—ã‚ã«å¼¾ã
                         if "Bs" not in title: 
                            continue

                    link = item.link.text
                    if link in seen_links:
                        continue
                    seen_links.add(link)

                    pub_date_str = item.pubDate.text
                    description = item.description.text
                    
                    # æ—¥ä»˜å‡¦ç† (JSTå¤‰æ›)
                    try:
                        timestamp = pd.to_datetime(pub_date_str)
                        if timestamp.tzinfo is None:
                            timestamp = timestamp.tz_localize('UTC')
                        else:
                            timestamp = timestamp.tz_convert('UTC')
                        timestamp_jst = timestamp.tz_convert('Asia/Tokyo')
                        display_date = timestamp_jst.strftime('%m/%d %H:%M')
                    except:
                        timestamp_jst = pd.Timestamp.now(tz='Asia/Tokyo')
                        display_date = pub_date_str

                    # è¦ç´„ã®ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°
                    summary_text = clean_summary(description)
                    if not summary_text:
                        summary_text = "è©³ç´°ã¯ã‚ã‚Šã¾ã›ã‚“"

                    # åª’ä½“åã®æŠ½å‡º
                    source = "News"
                    clean_title = title
                    if " - " in title:
                        parts = title.rsplit(" - ", 1)
                        clean_title = parts[0]
                        source = parts[1]

                    # ã‚«ãƒ†ã‚´ãƒªåˆ¤å®š (ã‚¿ã‚¤ãƒˆãƒ«ã¨è¦ç´„ã®ä¸¡æ–¹ã‚’ä½¿ã£ã¦åˆ¤å®š)
                    category = assign_category(clean_title + summary_text)

                    all_news_list.append({
                        "timestamp": timestamp_jst,
                        "date": display_date,
                        "category": category,
                        "media": source,
                        "title": clean_title,
                        "summary": summary_text,
                        "link": link,
                    })
                
                time.sleep(0.5)

            except Exception as e:
                print(f"Query '{query}' failed: {e}")
                continue

    if not all_news_list:
        return pd.DataFrame([
            {"timestamp": pd.Timestamp.now(), "date": "-", "category": "Error", "media": "-", "title": "ãƒ‡ãƒ¼ã‚¿å–å¾—ã‚¨ãƒ©ãƒ¼", "summary": "å†èª­ã¿è¾¼ã¿ã—ã¦ãã ã•ã„ã€‚", "link": "#"}
        ])

    df = pd.DataFrame(all_news_list)
    df = df.sort_values("timestamp", ascending=False).reset_index(drop=True)
    
    # é‡è¤‡æ’é™¤ (ã‚¿ã‚¤ãƒˆãƒ«é¡ä¼¼åº¦)
    unique_indices = []
    titles = df["title"].tolist()
    for i in range(len(titles)):
        is_duplicate = False
        for j in unique_indices:
            similarity = difflib.SequenceMatcher(None, titles[i], titles[j]).ratio()
            if similarity > 0.6: 
                is_duplicate = True
                break
        if not is_duplicate:
            unique_indices.append(i)
    
    df = df.iloc[unique_indices].reset_index(drop=True)
        
    return df

df = load_data()

# --- 2. ã‚µã‚¤ãƒ‰ãƒãƒ¼ ---
st.sidebar.title("ğŸ” æ¤œç´¢ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼")

sort_order = st.sidebar.radio("ä¸¦ã³é †", ["æ–°ã—ã„é †", "å¤ã„é †"], horizontal=True)

if sort_order == "å¤ã„é †":
    df = df.sort_values("timestamp", ascending=True).reset_index(drop=True)
else:
    df = df.sort_values("timestamp", ascending=False).reset_index(drop=True)

if not df.empty:
    categories = sorted(df["category"].unique())
    # "ãã®ä»–ãƒ‹ãƒ¥ãƒ¼ã‚¹"ã‚’æœ€å¾Œã«
    if "ãã®ä»–ãƒ‹ãƒ¥ãƒ¼ã‚¹" in categories:
        categories.remove("ãã®ä»–ãƒ‹ãƒ¥ãƒ¼ã‚¹")
        categories.append("ãã®ä»–ãƒ‹ãƒ¥ãƒ¼ã‚¹")

    selected_categories = st.sidebar.multiselect(
        "ãƒˆãƒ”ãƒƒã‚¯", categories, default=categories
    )
    
    search_query = st.sidebar.text_input("ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢")
    
    filtered_df = df[df["category"].isin(selected_categories)]
    
    if search_query:
        filtered_df = filtered_df[
            filtered_df["title"].str.contains(search_query, case=False) | 
            filtered_df["summary"].str.contains(search_query, case=False)
        ]
else:
    filtered_df = df

# --- 3. ãƒ¡ã‚¤ãƒ³ç”»é¢ ---
st.title("âš¾ ã‚ªãƒªãƒƒã‚¯ã‚¹ãƒ»ãƒãƒ•ã‚¡ãƒ­ãƒ¼ã‚º æœ€æ–°ãƒ‹ãƒ¥ãƒ¼ã‚¹")
st.caption("æœ€æ–°ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’è‡ªå‹•åé›†ã—ã¦è¡¨ç¤ºã—ã¦ã„ã¾ã™")

if st.button("ğŸ”„ ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’æ›´æ–°"):
    load_data.clear()
    st.rerun()

st.markdown("---")

if not filtered_df.empty:
    for index, row in filtered_df.iterrows():
        # ã‚«ãƒ†ã‚´ãƒªã«å¿œã˜ãŸCSSã‚¯ãƒ©ã‚¹
        cat_class = "cat-other"
        if row['category'] == "å¥‘ç´„ãƒ»ç§»ç±": cat_class = "cat-contract"
        elif row['category'] == "æ€ªæˆ‘ãƒ»èª¿æ•´": cat_class = "cat-injury"
        elif row['category'] == "çƒå›£ãƒ»ã‚¤ãƒ™ãƒ³ãƒˆ": cat_class = "cat-event"
        elif row['category'] == "ã‚­ãƒ£ãƒ³ãƒ—ãƒ»ç·´ç¿’": cat_class = "cat-camp"

        link_url = row['link']
        
        st.markdown(f"""
        <div class="news-card">
            <div class="news-header">
                <span class="news-category {cat_class}">{row['category']}</span>
                <span class="news-meta">ğŸ“… {row['date']} | ğŸ¢ {row['media']}</span>
            </div>
            <a href="{link_url}" target="_blank" class="news-title">{row['title']}</a>
            <div class="news-summary">{row['summary']}</div>
        </div>
        """, unsafe_allow_html=True)

else:
    st.warning("æ¡ä»¶ã«ä¸€è‡´ã™ã‚‹ãƒ‹ãƒ¥ãƒ¼ã‚¹ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")

# --- 4. ãƒ•ãƒƒã‚¿ãƒ¼ ---
st.markdown("---")
st.caption("Powered by Google News RSS")
